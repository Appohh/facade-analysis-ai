{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9025de8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6e1b4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\alpay\\Dev_projects\\Nelissen Project\\facade-analysis-ai\\notebooks\n",
      "Using device: cuda\n",
      "GPU Name: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "GPU Memory Allocated: 0.68 GB\n"
     ]
    }
   ],
   "source": [
    "# Print current working directory and device for debugging\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory Allocated: {torch.cuda.memory_allocated(0)/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f15555b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\alpay\\Dev_projects\\Nelissen Project\\facade-analysis-ai\\notebooks\n",
      "Image root path: ..\\data\\cmp_facade_dataset\\images\n",
      "Mask root path: ..\\data\\cmp_facade_dataset\\annotations\n",
      "Image train path exists: True\n",
      "Mask train path exists: True\n"
     ]
    }
   ],
   "source": [
    "# Print current working directory for debugging\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# --- Paths ---\n",
    "root_dir = Path(\"../data/cmp_facade_dataset\")\n",
    "image_root = root_dir / \"images\"\n",
    "mask_root = root_dir / \"annotations\"\n",
    "\n",
    "# Verify paths exist\n",
    "print(f\"Image root path: {image_root}\")\n",
    "print(f\"Mask root path: {mask_root}\")\n",
    "print(f\"Image train path exists: {(image_root / 'train').exists()}\")\n",
    "print(f\"Mask train path exists: {(mask_root / 'train').exists()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5f8e1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class with Resizing\n",
    "class CMPFacadeDataset(Dataset):\n",
    "    def __init__(self, image_dir, annot_dir, transform=None, resize_size=(512, 512)):\n",
    "        self.image_dir = image_dir\n",
    "        self.annot_dir = annot_dir\n",
    "        self.transform = transform\n",
    "        self.resize_size = resize_size\n",
    "        \n",
    "        # List image and annotation files\n",
    "        print(f\"Listing files in {image_dir}\")\n",
    "        image_files = sorted([f for f in os.listdir(image_dir) if f.endswith(\".png\")])\n",
    "        print(f\"Image files: {image_files[:5]} (total: {len(image_files)})\")\n",
    "        \n",
    "        print(f\"Listing files in {annot_dir}\")\n",
    "        annot_files = sorted([f for f in os.listdir(annot_dir) if f.endswith(\".png\")])\n",
    "        print(f\"Annotation files: {annot_files[:5]} (total: {len(annot_files)})\")\n",
    "        \n",
    "        # Pair files by extracting indices (e.g., image_0.png with annotation_0.png)\n",
    "        self.image_files = []\n",
    "        self.annot_files = []\n",
    "        for img_file in image_files:\n",
    "            img_idx = img_file.replace(\"image_\", \"\").replace(\".png\", \"\")\n",
    "            annot_file = f\"annotation_{img_idx}.png\"\n",
    "            if annot_file in annot_files:\n",
    "                self.image_files.append(img_file)\n",
    "                self.annot_files.append(annot_file)\n",
    "            else:\n",
    "                print(f\"Warning: No matching annotation for {img_file}\")\n",
    "        \n",
    "        if not self.image_files:\n",
    "            raise ValueError(f\"No matching image-annotation pairs found in {image_dir} and {annot_dir}\")\n",
    "        print(f\"Found {len(self.image_files)} image-annotation pairs in {image_dir}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "            annot_path = os.path.join(self.annot_dir, self.annot_files[idx])\n",
    "            \n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            annotation = Image.open(annot_path)\n",
    "            \n",
    "            # Resize image and annotation to the same size\n",
    "            image = image.resize(self.resize_size, Image.Resampling.LANCZOS)\n",
    "            annotation = annotation.resize(self.resize_size, Image.Resampling.NEAREST)  # NEAREST for masks to avoid interpolation artifacts\n",
    "            \n",
    "            image = np.array(image)\n",
    "            annotation = np.array(annotation)\n",
    "            \n",
    "            # Ensure class IDs are in range 0-11 (background as 0, classes 1-11)\n",
    "            annotation = np.clip(annotation, 0, 11)\n",
    "            \n",
    "            # Convert to tensor\n",
    "            image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
    "            annotation = torch.from_numpy(annotation).long()\n",
    "            \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            \n",
    "            return image, annotation\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {self.image_files[idx]}: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3110b78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing files in ..\\data\\cmp_facade_dataset\\images\\train\n",
      "Image files: ['image_0.png', 'image_1.png', 'image_10.png', 'image_100.png', 'image_101.png'] (total: 378)\n",
      "Listing files in ..\\data\\cmp_facade_dataset\\annotations\\train\n",
      "Annotation files: ['annotation_0.png', 'annotation_1.png', 'annotation_10.png', 'annotation_100.png', 'annotation_101.png'] (total: 378)\n",
      "Found 378 image-annotation pairs in ..\\data\\cmp_facade_dataset\\images\\train\n",
      "Listing files in ..\\data\\cmp_facade_dataset\\images\\eval\n",
      "Image files: ['image_0.png', 'image_1.png', 'image_10.png', 'image_100.png', 'image_101.png'] (total: 114)\n",
      "Listing files in ..\\data\\cmp_facade_dataset\\annotations\\eval\n",
      "Annotation files: ['annotation_0.png', 'annotation_1.png', 'annotation_10.png', 'annotation_100.png', 'annotation_101.png'] (total: 114)\n",
      "Found 114 image-annotation pairs in ..\\data\\cmp_facade_dataset\\images\\eval\n",
      "Listing files in ..\\data\\cmp_facade_dataset\\images\\test\n",
      "Image files: ['image_0.png', 'image_1.png', 'image_10.png', 'image_100.png', 'image_101.png'] (total: 114)\n",
      "Listing files in ..\\data\\cmp_facade_dataset\\annotations\\test\n",
      "Annotation files: ['annotation_0.png', 'annotation_1.png', 'annotation_10.png', 'annotation_100.png', 'annotation_101.png'] (total: 114)\n",
      "Found 114 image-annotation pairs in ..\\data\\cmp_facade_dataset\\images\\test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alpay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\alpay\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Create Datasets with resizing\n",
    "train_dataset = CMPFacadeDataset(image_root / \"train\", mask_root / \"train\", transform=None, resize_size=(512, 512))\n",
    "eval_dataset = CMPFacadeDataset(image_root / \"eval\", mask_root / \"eval\", resize_size=(512, 512))\n",
    "test_dataset = CMPFacadeDataset(image_root / \"test\", mask_root / \"test\", resize_size=(512, 512))\n",
    "\n",
    "# Create Data Loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=4, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=0)\n",
    "\n",
    "# Load DeepLabv3 Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=True)\n",
    "\n",
    "# Modify the classifier for 12 classes (0-11, including background)\n",
    "model.classifier[4] = nn.Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff8bb43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 1/10, Total Batches: 95\n",
      "Processing Batch 1/95\n",
      "Batch 1/95, Loss: 1.3733\n",
      "Processing Batch 2/95\n",
      "Batch 2/95, Loss: 1.1901\n",
      "Processing Batch 3/95\n",
      "Batch 3/95, Loss: 1.2441\n",
      "Processing Batch 4/95\n",
      "Batch 4/95, Loss: 1.1750\n",
      "Processing Batch 5/95\n",
      "Batch 5/95, Loss: 1.2539\n",
      "Processing Batch 6/95\n",
      "Batch 6/95, Loss: 0.9911\n",
      "Processing Batch 7/95\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     18\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 20\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_batches\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training Loop with Progress Logging\n",
    "num_epochs = 10\n",
    "best_loss = float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_batches = len(train_loader)\n",
    "    print(f\"Starting Epoch {epoch+1}/{num_epochs}, Total Batches: {total_batches}\")\n",
    "    \n",
    "    for batch_idx, (images, annotations) in enumerate(train_loader):\n",
    "        print(f\"Processing Batch {batch_idx+1}/{total_batches}\")\n",
    "        images, annotations = images.to(device), annotations.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)['out']\n",
    "        loss = criterion(outputs, annotations)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        print(f\"Batch {batch_idx+1}/{total_batches}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    total_val_batches = len(eval_loader)\n",
    "    print(f\"Starting Validation, Total Batches: {total_val_batches}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, annotations) in enumerate(eval_loader):\n",
    "            print(f\"Validation Batch {batch_idx+1}/{total_val_batches}\")\n",
    "            images, annotations = images.to(device), annotations.to(device)\n",
    "            outputs = model(images)['out']\n",
    "            loss = criterion(outputs, annotations)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    val_loss = val_loss / len(eval_loader.dataset)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"deeplabv3_best_model.pth\")\n",
    "        print(\"Saved best model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a99161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "model.load_state_dict(torch.load(\"deeplabv3_best_model.pth\"))\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for images, annotations in test_loader:\n",
    "        images, annotations = images.to(device), annotations.to(device)\n",
    "        outputs = model(images)['out']\n",
    "        loss = criterion(outputs, annotations)\n",
    "        test_loss += loss.item() * images.size(0)\n",
    "\n",
    "test_loss = test_loss / len(test_loader.dataset)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Visualize a Test Prediction\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    images, annotations = next(iter(test_loader))\n",
    "    images, annotations = images.to(device), annotations.to(device)\n",
    "    outputs = model(images)['out']\n",
    "    preds = torch.argmax(outputs, dim=1)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(images[0].cpu().permute(1, 2, 0).numpy())\n",
    "    plt.title(\"Image\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(annotations[0].cpu().numpy(), cmap=\"tab20\", vmin=0, vmax=11)\n",
    "    plt.title(\"Ground Truth\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(preds[0].cpu().numpy(), cmap=\"tab20\", vmin=0, vmax=11)\n",
    "    plt.title(\"Prediction\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Script executed successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
